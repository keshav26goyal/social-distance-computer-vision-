{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USAGE\n",
    "# python social_distance_detector.py --input pedestrians.mp4\n",
    "# python social_distance_detector.py --input pedestrians.mp4 --output output.avi\n",
    "\n",
    "# import the necessary packages\n",
    "from pyimagesearch import social_distancing_config as config\n",
    "from pyimagesearch.detection import detect_people\n",
    "from scipy.spatial import distance as dist\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import cv2\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading YOLO from disk...\n",
      "[INFO] accessing video stream...\n"
     ]
    }
   ],
   "source": [
    "# construct the argument parse and parse the arguments\n",
    "# --input: The path to the optional video file. If no video file path is provided, your computer’s first webcam will be used by default.\n",
    "#--output : The optional path to an output (i.e., processed) video file. If this argument is not provided, the processed video will not be exported to disk.\n",
    "#--display   : By default, we’ll display our social distance application on-screen as we process each frame. Alternatively, you can set this value to 0to process the stream in the background.\n",
    "\n",
    "\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-i\", \"--input\", type=str, default=\"\",\n",
    "\thelp=\"path to (optional) input video file\")\n",
    "ap.add_argument(\"-o\", \"--output\", type=str, default=\"\",\n",
    "\thelp=\"path to (optional) output video file\")\n",
    "ap.add_argument(\"-d\", \"--display\", type=int, default=1,\n",
    "\thelp=\"whether or not output frame should be displayed\")\n",
    "args = vars(ap.parse_args([\"--input\",\"/C:/Users/Goyal/pedestrians.mp4\",\"--output\",\"output.avi\",\"--display\",\"1\"]))\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# load the COCO class labels our YOLO model was trained on\n",
    "labelsPath = os.path.sep.join([config.MODEL_PATH, \"coco.names\"])\n",
    "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "\n",
    "# derive the paths to the YOLO weights and model configuration\n",
    "weightsPath = os.path.sep.join([config.MODEL_PATH, \"yolov3.weights\"])\n",
    "configPath = os.path.sep.join([config.MODEL_PATH, \"yolov3.cfg\"])\n",
    "#---------------------------------------------------------------------------------------------------------------------#\n",
    "#Using OpenCV’s DNN module, we load our YOLO net into memory.\n",
    "#If you have the USE_GPU option set in the config, then the backend processor is set to be your NVIDIA CUDA-capable GPU.\n",
    "#If you don’t have a CUDA-capable GPU, ensure that the configuration option is set to False so that your CPU is the processor used.\n",
    "\n",
    "\n",
    "# load our YOLO object detector trained on COCO dataset (80 classes)\n",
    "print(\"[INFO] loading YOLO from disk...\")\n",
    "net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\n",
    "# check if we are going to use GPU\n",
    "if config.USE_GPU:\n",
    "\t# set CUDA as the preferable backend and target\n",
    "\tprint(\"[INFO] setting preferable backend and target to CUDA...\")\n",
    "\tnet.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "\tnet.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "#----------------------------------------------------------------------------------------------------------------------#\n",
    "#Here,gather the output layer names from YOLO; we’ll need them in order to process our results.\n",
    "#We then start our video stream (either a video file via the --input command line argument or a webcam stream) \n",
    "#For now, we initialize our output video writer to None .Further setup occurs in the frame processing loop.\n",
    "#Finally, we’re ready to begin processing frames and determining if people are maintaining safe social distance:\n",
    "\n",
    "# determine only the *output* layer names that we need from YOLO\n",
    "ln = net.getLayerNames()\n",
    "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# initialize the video stream and pointer to output video file\n",
    "print(\"[INFO] accessing video stream...\")\n",
    "vs = cv2.VideoCapture(args[\"input\"] if args[\"input\"] else 0)\n",
    "writer = None\n",
    "#----------------------------------------------------------------------------------------------------------------------#\n",
    "#begins a loop over frames from our video stream.\n",
    "#The dimensions of our input video for testing are quite large, so we resize each frame while maintaining aspect ratio\n",
    "#Using our detect_people function implemented in the previous section, we grab results of YOLO object detection. If you need a refresher on the input parameters required or the format of the output results for the function call, be sure to refer to the listing in the previous section.\n",
    "#We then initialize our violate set. this set maintains a listing of people who violate social distance regulations set forth by public health professionals.\n",
    "\n",
    "\n",
    "\n",
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "\t# read the next frame from the file\n",
    "\t(grabbed, frame) = vs.read()\n",
    "\n",
    "\t# if the frame was not grabbed, then we have reached the end\n",
    "\t# of the stream\n",
    "\tif not grabbed:\n",
    "\t\tbreak\n",
    "\n",
    "\t# resize the frame and then detect people (and only people) in it\n",
    "\tframe = imutils.resize(frame, width=700)\n",
    "\tresults = detect_people(frame, net, ln,\n",
    "\t\tpersonIdx=LABELS.index(\"person\"))\n",
    "\n",
    "\t# initialize the set of indexes that violate the minimum social\n",
    "\t# distance\n",
    "\tviolate = set()\n",
    "#-------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#Assuming that at least two people were detected in the frame  we proceed to:\n",
    "#Compute the Euclidean distance between all pairs of centroids \n",
    "#Loop over the upper triangular of distance matrix (since the matrix is symmetrical) \n",
    "#Check to see if the distance violates our minimum social distance set forth by public health professionals .If two people are too close, we add them to the violate set\n",
    "\n",
    "\n",
    "\n",
    "\t# ensure there are *at least* two people detections (required in\n",
    "\t# order to compute our pairwise distance maps)\n",
    "\tif len(results) >= 2:\n",
    "\t\t# extract all centroids from the results and compute the\n",
    "\t\t# Euclidean distances between all pairs of the centroids\n",
    "\t\tcentroids = np.array([r[2] for r in results])\n",
    "\t\tD = dist.cdist(centroids, centroids, metric=\"euclidean\")\n",
    "\n",
    "\t\t# loop over the upper triangular of the distance matrix\n",
    "\t\tfor i in range(0, D.shape[0]):\n",
    "\t\t\tfor j in range(i + 1, D.shape[1]):\n",
    "\t\t\t\t# check to see if the distance between any two\n",
    "\t\t\t\t# centroid pairs is less than the configured number\n",
    "\t\t\t\t# of pixels\n",
    "\t\t\t\tif D[i, j] < config.MIN_DISTANCE:\n",
    "\t\t\t\t\t# update our violation set with the indexes of\n",
    "\t\t\t\t\t# the centroid pairs\n",
    "\t\t\t\t\tviolate.add(i)\n",
    "\t\t\t\t\tviolate.add(j)\n",
    "#---------------------------------------------------------------------------------------------------------------------#\n",
    "# Looping over the results, we proceed to:\n",
    "#Extract the bounding box and centroid coordinates \n",
    "#Initialize the color of the bounding box to green \n",
    "#Check to see if the current index exists in our violate set, and if so, update the color to red\n",
    "# Draw both the bounding box of the person and their object centroid . Each is color -coordinated, so we’ll see which people are too close.\n",
    "# Display information on the total number of social distancing violations (the length of our violate set (Lines 108-110)\n",
    "\n",
    "\t# loop over the results\n",
    "\tfor (i, (prob, bbox, centroid)) in enumerate(results):\n",
    "\t\t# extract the bounding box and centroid coordinates, then\n",
    "\t\t# initialize the color of the annotation\n",
    "\t\t(startX, startY, endX, endY) = bbox\n",
    "\t\t(cX, cY) = centroid\n",
    "\t\tcolor = (0, 255, 0)\n",
    "\n",
    "\t\t# if the index pair exists within the violation set, then\n",
    "\t\t# update the color\n",
    "\t\tif i in violate:\n",
    "\t\t\tcolor = (0, 0, 255)\n",
    "\n",
    "\t\t# draw (1) a bounding box around the person and (2) the\n",
    "\t\t# centroid coordinates of the person,\n",
    "\t\tcv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
    "\t\tcv2.circle(frame, (cX, cY), 5, color, 1)\n",
    "\n",
    "\t# draw the total number of social distancing violations on the\n",
    "\t# output frame\n",
    "\ttext = \"Social Distancing Violations: {}\".format(len(violate))\n",
    "\tcv2.putText(frame, text, (10, frame.shape[0] - 25),\n",
    "\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.85, (0, 0, 255), 3)\n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "#To close out, we:\n",
    "#Display the frame to the screen if required while waiting for the q (quit) key to be pressed \n",
    "#Initialize our video writer if necessary \n",
    "#Write the processed (annotated) frame to disk \n",
    "\n",
    "\n",
    "\t# check to see if the output frame should be displayed to our\n",
    "\t# screen\n",
    "\tif args[\"display\"] > 0:\n",
    "\t\t# show the output frame\n",
    "\t\tcv2.imshow(\"Frame\", frame)\n",
    "\t\tkey = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "\t\t# if the `q` key was pressed, break from the loop\n",
    "\t\tif key == ord(\"q\"):\n",
    "\t\t\tbreak\n",
    "\n",
    "\t# if an output video file path has been supplied and the video\n",
    "\t# writer has not been initialized, do so now\n",
    "\tif args[\"output\"] != \"\" and writer is None:\n",
    "\t\t# initialize our video writer\n",
    "\t\tfourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "\t\twriter = cv2.VideoWriter(args[\"output\"], fourcc, 25,\n",
    "\t\t\t(frame.shape[1], frame.shape[0]), True)\n",
    "\n",
    "\t# if the video writer is not None, write the frame to the output\n",
    "\t# video file\n",
    "\tif writer is not None:\n",
    "\t\twriter.write(frame)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
